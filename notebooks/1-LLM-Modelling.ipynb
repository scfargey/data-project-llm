{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Proper Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from gensim.models import CoherenceModel\n",
    "from datasets import Dataset, DatasetDict\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('SetFit/20_newsgroups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into test and train dataframes\n",
    "df_train = ds['train'].to_pandas()\n",
    "df_test = ds['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the splits\n",
    "train = Dataset.from_pandas(df_train)\n",
    "test = Dataset.from_pandas(df_test)\n",
    "# reconstruct both datasets into a Dataset Dict object\n",
    "new_ds = DatasetDict(\n",
    "    {\n",
    "        'train': train,\n",
    "        'test': test\n",
    "    }\n",
    ")\n",
    "# view the resulting dataset dict object\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre processing function\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation and non-alphanumeric characters, including digits\n",
    "    text = re.sub(r'\\W+|\\d+', ' ', text)  # This removes both punctuation and numbers\n",
    "\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove single-character tokens (except 'a' and 'i')\n",
    "    tokens = [word for word in tokens if len(word) > 1 or word in ['a', 'i']]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join tokens back into a single string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "\n",
    "    # Return None if the document is empty after preprocessing\n",
    "    return cleaned_text if len(cleaned_text.strip()) > 0 else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the text data in train and test sets\n",
    "df_train['cleaned_text'] = df_train['text'].apply(preprocess_text)\n",
    "df_test['cleaned_text'] = df_test['text'].apply(preprocess_text)\n",
    "\n",
    "# Remove empty (None) documents\n",
    "df_train = df_train[df_train['cleaned_text'].notnull()]\n",
    "df_test = df_test[df_test['cleaned_text'].notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the cleaned data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data: Transforming data into a format that LDA can work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the cleaned text\n",
    "df_train['tokenized_text'] = df_train['cleaned_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Create dictionary for LDA\n",
    "dictionary = corpora.Dictionary(df_train['tokenized_text'])\n",
    "\n",
    "# Create Bag of Words corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in df_train['tokenized_text']]\n",
    "\n",
    "# View a sample from the corpus (word ID and count)\n",
    "print(corpus[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying LDA Model and view topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LDA model\n",
    "lda_model = gensim.models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=8,\n",
    "    random_state=42,\n",
    "    chunksize=100,\n",
    "    passes=15,\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "# View the 10 topics and their top words\n",
    "lda_model.print_topics(num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Model with coherance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's coherence\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=df_train['tokenized_text'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'\\nCoherence Score: {coherence_lda}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try a NMF (Non-Negative Matrix Factrorization) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "# Fit and transform the cleaned text data to create the TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_train['cleaned_text'])\n",
    "\n",
    "# The matrix is now ready to be used in models like LSA or NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "# Fit and transform the cleaned text data to create the TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_train['cleaned_text'])\n",
    "\n",
    "# Get the feature names (terms) from the TF-IDF vectorizer\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Apply NMF on the TF-IDF matrix\n",
    "nmf_model = NMF(n_components=10, random_state=42)\n",
    "nmf_model.fit(tfidf_matrix)\n",
    "\n",
    "# Print the top words for each topic\n",
    "for i, topic in enumerate(nmf_model.components_):\n",
    "    terms_in_topic = [terms[x] for x in topic.argsort()[:-10 - 1:-1]]\n",
    "    print(f\"Topic {i}: {', '.join(terms_in_topic)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply a Pre-Trained LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import pipeline and use pre-trained model focused on news category classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Set up the pipeline for topic classification with a pre-trained model\n",
    "classifier = pipeline(\"text-classification\", model=\"Yueh-Huan/news-category-classification-distilbert\", device=0)\n",
    "\n",
    "# Get predictions for your dataset\n",
    "dataset_texts = df_train['cleaned_text'].tolist()\n",
    "\n",
    "# Get predictions from the classifier using the actual dataset\n",
    "predictions = classifier(dataset_texts, batch_size=16, truncation=True)\n",
    "\n",
    "# Add predictions to the dataframe (optional)\n",
    "df_train['predicted_topic'] = [pred['label'] for pred in predictions]\n",
    "\n",
    "# Inspect the first few rows of the updated dataframe\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset with the predicted topic so we can look at it wiithout running again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the df_train dataset with the predicted topics\n",
    "df_train.to_csv('df_train_with_predicted_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this file to my local desktop\n",
    "from google.colab import files\n",
    "files.download('df_train_with_predicted_topics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
