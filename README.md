# LLM Project

## Project Task
In this project, I am working on topic modeling for the 20 Newsgroups dataset. The main objective is to classify text documents into predefined categories based on their content, leveraging a pre-trained Large Language Model (LLM) for efficient topic classification.

## Dataset
I am using the 20 Newsgroups dataset, which contains approximately 20,000 newsgroup documents across 20 topics. Each document is associated with a specific label representing its topic, such as "rec.autos," "comp.sys.mac.hardware," "sci.space," and others. This dataset is a well-known benchmark for text classification tasks and provides a variety of topics that span multiple domains, making it suitable for topic modeling.

## Pre-trained Model
For this project, I used the DistilBERT model, specifically the fine-tuned version Yueh-Huan/news-category-classification-distilbert, available on Hugging Face. DistilBERT is a smaller, faster, and more efficient version of BERT (Bidirectional Encoder Representations from Transformers), which is well-suited for tasks like text classification due to its reduced computational requirements without significantly compromising performance.

## Performance Metrics
To evaluate the model's performance, I used the coherence score, which measures the interpretability and relevance of the topics generated by the model. The coherence score achieved was approximately 0.62, indicating moderate interpretability of the topic clusters. This score reflects the model's ability to group similar documents together meaningfully.

In addition to the coherence score, I also reviewed the predicted labels for accuracy in capturing the intended categories, ensuring that they align closely with the original topics.

## Hyperparameters
The most important hyperparameters for optimizing the model included:

Batch Size: A batch size of 16 was chosen to balance memory usage and processing speed.
Max Sequence Length (Truncation): Setting truncation=True to handle longer text documents without exceeding model limits.
Device Configuration: Specifying device=0 for GPU usage significantly improved inference speed.
These hyperparameters were tuned to optimize the performance and efficiency of the model for large datasets like 20 Newsgroups. Further tuning, particularly of batch size and sequence length, could improve both performance and coherence.

